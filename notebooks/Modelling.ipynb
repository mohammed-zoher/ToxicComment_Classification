{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IOD86nP1n7ec"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from prettytable import PrettyTable\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxk-kq3rn7ey"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGXk5NLNn7ey"
   },
   "source": [
    "Now that we have performed exploratory analysis on our dataset, applied the necessary processing functions and loaded text feature representations we can go ahead with the modelling phase. Under this section we will be experimenting with various deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYCOUFDDn7ey"
   },
   "outputs": [],
   "source": [
    "# loading tokenized data \n",
    "with open(\"resources/tokenized_data.pkl\",\"rb\") as f:\n",
    "    x_train, y_train, x_cv, y_cv, x_test = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf2fFxMSn7ez"
   },
   "source": [
    "## Custom Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0Kh0YODn7ez"
   },
   "outputs": [],
   "source": [
    "# custom callback for performance metric: mean column wise AUC\n",
    "class CustomMetrics(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self,train_data,train_labels,val_data,val_labels):\n",
    "        '''\n",
    "        This function initializes callback object to \n",
    "        compute custom metric\n",
    "        '''\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.val_data = val_data\n",
    "        self.val_labels = val_labels\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        '''\n",
    "        This function computes the mean wise column AUC at \n",
    "        the end of each epoch\n",
    "        '''\n",
    "        \n",
    "        # predicting probabilities for training datapoints\n",
    "        train_proba = self.model.predict(self.train_data)\n",
    "        \n",
    "        # mean column wise auc for train set\n",
    "        train_auc = roc_auc_score(y_true=self.train_labels,\n",
    "                                  y_score=train_proba,\n",
    "                                  average=\"macro\")\n",
    "        \n",
    "        # predicting probabilities for val datapoints\n",
    "        val_proba = self.model.predict(self.val_data)\n",
    "        \n",
    "        # mean column wise auc for val set\n",
    "        val_auc = roc_auc_score(y_true=self.val_labels,\n",
    "                                  y_score=val_proba,\n",
    "                                  average=\"macro\")\n",
    "        \n",
    "        print(f\"train_auc: {round(train_auc,4)} val_auc: {round(val_auc,4)}\")\n",
    "\n",
    "\n",
    "# custom callback to save model after each epoch\n",
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self,file_path):\n",
    "        '''\n",
    "        This function initializes callback object to \n",
    "        save model\n",
    "        '''\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        '''\n",
    "        Function saves model architecture, weights and optimizer state for current epoch\n",
    "        '''\n",
    "\n",
    "        # saving the model to specified file location\n",
    "        self.model.save(self.file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm9OhRUgo_Sm"
   },
   "source": [
    "## Defining Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0fBkmOEMPp6"
   },
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zg_LPI5pDX6"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_type):\n",
    "    '''\n",
    "    Function to load embeddings based on input type specified\n",
    "    '''\n",
    "\n",
    "    # creating file path\n",
    "    fp = f\"resources/{embedding_type}_embedding_matrix.pkl\"\n",
    "\n",
    "    # loading embedding matrix\n",
    "    with open(fp,mode=\"rb\") as f:\n",
    "      embedding_matrix = pickle.load(file=f)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "def get_cnn_architecture(max_length,vocab_size,embedding_matrix):\n",
    "    '''\n",
    "    Function creates CNN architecture with 1d conv layers\n",
    "    '''\n",
    "\n",
    "    # clearing backend session\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # defining kernel initializer and regularizer\n",
    "    initializer = HeNormal()\n",
    "    regularizer = L2(l2=0.01)\n",
    "\n",
    "    # defining input and embedding layers\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    embedding = Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length,weights=[embedding_matrix],trainable=False)(input_layer)\n",
    "\n",
    "    # defining the first set of conv1d layers\n",
    "    conv_a_1 = Conv1D(50,3,1,activation='relu',kernel_initializer=initializer,padding='same')(embedding)\n",
    "    conv_a_2 = Conv1D(50,4,1,activation='relu',kernel_initializer=initializer,padding='same')(embedding)\n",
    "    conv_a_3 = Conv1D(50,5,1,activation='relu',kernel_initializer=initializer,padding='same')(embedding)\n",
    "\n",
    "    # concatenating and max pool first set of conv1d layers\n",
    "    concat_a = concatenate([conv_a_1,conv_a_2,conv_a_3])\n",
    "    maxpool_a = MaxPooling1D(pool_size=2,strides=1)(concat_a)\n",
    "\n",
    "    # defining the second set of conv1d layers\n",
    "    conv_b_1 = Conv1D(50,3,1,activation='relu',kernel_initializer=initializer,padding='same')(maxpool_a)\n",
    "    conv_b_2 = Conv1D(50,4,1,activation='relu',kernel_initializer=initializer,padding='same')(maxpool_a)\n",
    "    conv_b_3 = Conv1D(50,5,1,activation='relu',kernel_initializer=initializer,padding='same')(maxpool_a)\n",
    "\n",
    "    # concatenating and max pool second set of conv1d layers\n",
    "    concat_b = concatenate([conv_b_1,conv_b_2,conv_b_3])\n",
    "    maxpool_b = MaxPooling1D(pool_size=2,strides=1)(concat_b)\n",
    "\n",
    "    # final conv1d layer and dense layers\n",
    "    conv_c = Conv1D(50,5,1,activation='relu',kernel_initializer=initializer)(maxpool_b)\n",
    "    flatten = Flatten()(conv_c)\n",
    "    drop_1 = Dropout(rate=0.5)(flatten)\n",
    "    dense_1 = Dense(units=32,activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizer)(drop_1)\n",
    "    output_layer = Dense(units=6,activation='sigmoid',kernel_initializer=initializer)(dense_1)\n",
    "\n",
    "    # creating the model\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "  \n",
    "\n",
    "def get_lstm_architecture(max_length,vocab_size,embedding_matrix):\n",
    "    '''\n",
    "    Function creates LSTM architecture with the input embedding matrix specified \n",
    "    '''\n",
    "\n",
    "    # clearing backend session\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # defining input and embedding layers\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    embedding = Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length,weights=[embedding_matrix],trainable=False)(input_layer) \n",
    "\n",
    "    # bi-directional lstm layers\n",
    "    lstm_output_1 = Bidirectional(LSTM(units=64,return_sequences=True))(embedding)\n",
    "    drop = Dropout(rate=0.5)(lstm_output_1)\n",
    "    lstm_output_2 = Bidirectional(LSTM(units=64,return_sequences=False))(drop)\n",
    "\n",
    "    # output layer\n",
    "    output_layer = Dense(units=6,activation='sigmoid')(lstm_output_2)\n",
    "\n",
    "    # creating the model\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_gru_architecture(max_length,vocab_size,embedding_matrix):\n",
    "    '''\n",
    "    Function creates GRU architecture with the input embedding matrix specified \n",
    "    '''\n",
    "\n",
    "    # clearing backend session\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # defining input and embedding layers\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    embedding = Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length,weights=[embedding_matrix],trainable=False)(input_layer) \n",
    "\n",
    "    # bi-directional GRU layers with MaxPooling1D\n",
    "    gru_output_1 = Bidirectional(GRU(units=64,return_sequences=True))(embedding)\n",
    "    max_pool = MaxPooling1D()(gru_output_1)\n",
    "    drop = Dropout(rate=0.5)(max_pool)\n",
    "    gru_output_2 = Bidirectional(GRU(units=64,return_sequences=False))(drop)\n",
    "\n",
    "    # output layer\n",
    "    output_layer = Dense(units=6,activation='sigmoid')(gru_output_2)\n",
    "\n",
    "    # creating the model\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_test_predictions(test_ids,test_data,model_type):\n",
    "    '''\n",
    "    Function returns predictions of test data set by using input model specified\n",
    "    '''\n",
    "\n",
    "    # model file path\n",
    "    fp = f\"models/{model_type}.hdf5\"\n",
    "\n",
    "    # loading the model\n",
    "    model = tf.keras.models.load_model(fp)\n",
    "\n",
    "    # predicting class probabilities\n",
    "    pred_proba = model.predict(test_data)\n",
    "\n",
    "    # dataframe to store results\n",
    "    pred_df = pd.DataFrame()\n",
    "\n",
    "    # saving ids\n",
    "    pred_df[\"id\"] = test_ids\n",
    "\n",
    "    # adding predicted probability for each class\n",
    "    class_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "    for i,label in enumerate(class_labels):\n",
    "      pred_df[label] = pred_proba[:,i]\n",
    "\n",
    "    \n",
    "    # filepath to save predictions\n",
    "    fp = f\"predictions/{model_type}.csv\"\n",
    "\n",
    "    # saving to disk\n",
    "    pred_df.to_csv(fp)\n",
    "\n",
    "    print(\"Predictions saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoFxKsXbn7ez"
   },
   "source": [
    "## CNN Model + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUOrZ0aTn7ez"
   },
   "outputs": [],
   "source": [
    "# loading the glove embeddings\n",
    "word_embedding_matrix = load_embeddings(embedding_type=\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nu3KB0bUn7ez",
    "outputId": "9b54cc33-6a35-4658-80a2-516ab2ed1560",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 300)     23813400    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 200, 50)      45050       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 200, 50)      60050       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 200, 50)      75050       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200, 150)     0           conv1d[0][0]                     \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 199, 150)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 199, 50)      22550       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 199, 50)      30050       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 199, 50)      37550       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 199, 150)     0           conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 198, 150)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 194, 50)      37550       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 9700)         0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 9700)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           310432      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            198         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 24,431,880\n",
      "Trainable params: 618,480\n",
      "Non-trainable params: 23,813,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_cnn_architecture(max_length=max_length,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_matrix=word_embedding_matrix)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmnRzaJbn7ez"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/cnn-glove.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_cv,\n",
    "                              val_labels=y_cv)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=2,\n",
    "                           verbose=1)\n",
    "save_model = SaveModel(file_path=filepath)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,save_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Dy5si8Gn7ez"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MdNfqChn7ez",
    "outputId": "4018cdbf-ca5c-474d-91bc-9159f50fb4a7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1995/1995 [==============================] - 113s 41ms/step - loss: 0.0825 - val_loss: 0.0643\n",
      "train_auc: 0.9655 val_auc: 0.9653\n",
      "Epoch 2/20\n",
      "1995/1995 [==============================] - 80s 40ms/step - loss: 0.0607 - val_loss: 0.0584\n",
      "train_auc: 0.9772 val_auc: 0.9716\n",
      "Epoch 3/20\n",
      "1995/1995 [==============================] - 79s 40ms/step - loss: 0.0551 - val_loss: 0.0614\n",
      "train_auc: 0.9835 val_auc: 0.9737\n",
      "Epoch 4/20\n",
      "1995/1995 [==============================] - 79s 40ms/step - loss: 0.0509 - val_loss: 0.0602\n",
      "train_auc: 0.9862 val_auc: 0.9725\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19e02a0650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(x_train,y_train,validation_data=(x_cv,y_cv),batch_size=64,epochs=20,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy3IoiOsuC8x",
    "outputId": "803d461e-0248-4c0a-b1d8-5103e3a0fabf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test,\n",
    "                     model_type=\"cnn-glove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixf17KsXOOqS"
   },
   "source": [
    "## CNN Model + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY8FKgwJOTmL"
   },
   "outputs": [],
   "source": [
    "# loading the fasttext embeddings\n",
    "word_embedding_matrix = load_embeddings(embedding_type=\"fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4lNAVzcOgK4",
    "outputId": "99067e67-6abd-4aa7-f05a-89289d36dfd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     23813400    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 200, 50)      45050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 200, 50)      60050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 200, 50)      75050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 150)     0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 199, 150)     0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 199, 50)      22550       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 199, 50)      30050       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 199, 50)      37550       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 199, 150)     0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 198, 150)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 194, 50)      37550       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9700)         0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 9700)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           310432      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            198         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,431,880\n",
      "Trainable params: 618,480\n",
      "Non-trainable params: 23,813,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_cnn_architecture(max_length=max_length,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_matrix=word_embedding_matrix)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xECOmzWxOjbo"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/cnn-fasttext.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_cv,\n",
    "                              val_labels=y_cv)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=2,\n",
    "                           verbose=1)\n",
    "save_model = SaveModel(file_path=filepath)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,save_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvIh57GGOmkX"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JznZVdtYOosa",
    "outputId": "ca24ade8-d385-44a7-afb4-a24b717314cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   6/1995 [..............................] - ETA: 1:32 - loss: 0.9692WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0196s vs `on_train_batch_end` time: 0.0231s). Check your callbacks.\n",
      "1995/1995 [==============================] - 112s 41ms/step - loss: 0.0829 - val_loss: 0.0671\n",
      "train_auc: 0.9722 val_auc: 0.9684\n",
      "Epoch 2/20\n",
      "1995/1995 [==============================] - 80s 40ms/step - loss: 0.0623 - val_loss: 0.0653\n",
      "train_auc: 0.9748 val_auc: 0.9669\n",
      "Epoch 3/20\n",
      "1995/1995 [==============================] - 80s 40ms/step - loss: 0.0566 - val_loss: 0.0625\n",
      "train_auc: 0.9811 val_auc: 0.9709\n",
      "Epoch 4/20\n",
      "1995/1995 [==============================] - 80s 40ms/step - loss: 0.0517 - val_loss: 0.0612\n",
      "train_auc: 0.983 val_auc: 0.9672\n",
      "Epoch 5/20\n",
      "1995/1995 [==============================] - 80s 40ms/step - loss: 0.0472 - val_loss: 0.0700\n",
      "train_auc: 0.9874 val_auc: 0.9707\n",
      "Epoch 6/20\n",
      "1995/1995 [==============================] - 81s 41ms/step - loss: 0.0440 - val_loss: 0.0650\n",
      "train_auc: 0.9878 val_auc: 0.9688\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f3071c790>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_cv,y_cv),batch_size=64,epochs=20,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uG58r7McRHBK",
    "outputId": "87536e36-9d1a-4147-f73e-9a1e40f8e319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test,\n",
    "                     model_type=\"cnn-fasttext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_DfwzziQnf7"
   },
   "source": [
    "## LSTM Model + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoLSqVALQrEt"
   },
   "outputs": [],
   "source": [
    "# loading the glove embeddings\n",
    "word_embedding_matrix = load_embeddings(embedding_type=\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_12dMAJlQwIQ",
    "outputId": "2282f4f6-5da4-43d4-b8dc-757753b34ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 300)          23813400  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 128)          186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 24,099,870\n",
      "Trainable params: 286,470\n",
      "Non-trainable params: 23,813,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_lstm_architecture(max_length=max_length,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_matrix=word_embedding_matrix)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTR8bWBKRR01"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/lstm-glove.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_cv,\n",
    "                              val_labels=y_cv)\n",
    "reduced_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                               patience=1,\n",
    "                               verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=2,\n",
    "                           verbose=1)\n",
    "save_model = SaveModel(file_path=filepath)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,save_model,reduced_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA7M_1YZRrrB"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGZC46EkRxNm",
    "outputId": "093594d7-2ffa-4024-f26a-192609d8316f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  6/998 [..............................] - ETA: 3:17 - loss: 0.5914WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0924s vs `on_train_batch_end` time: 0.0964s). Check your callbacks.\n",
      "998/998 [==============================] - 169s 158ms/step - loss: 0.0664 - val_loss: 0.0498\n",
      "train_auc: 0.976 val_auc: 0.9735\n",
      "Epoch 2/10\n",
      "998/998 [==============================] - 157s 157ms/step - loss: 0.0481 - val_loss: 0.0462\n",
      "train_auc: 0.9833 val_auc: 0.981\n",
      "Epoch 3/10\n",
      "998/998 [==============================] - 155s 155ms/step - loss: 0.0444 - val_loss: 0.0456\n",
      "train_auc: 0.9871 val_auc: 0.9829\n",
      "Epoch 4/10\n",
      "998/998 [==============================] - 157s 157ms/step - loss: 0.0419 - val_loss: 0.0452\n",
      "train_auc: 0.9893 val_auc: 0.9838\n",
      "Epoch 5/10\n",
      "998/998 [==============================] - 156s 157ms/step - loss: 0.0397 - val_loss: 0.0441\n",
      "train_auc: 0.9912 val_auc: 0.9854\n",
      "Epoch 6/10\n",
      "998/998 [==============================] - 156s 156ms/step - loss: 0.0375 - val_loss: 0.0439\n",
      "train_auc: 0.9925 val_auc: 0.9851\n",
      "Epoch 7/10\n",
      "998/998 [==============================] - 156s 156ms/step - loss: 0.0355 - val_loss: 0.0457\n",
      "train_auc: 0.9935 val_auc: 0.9839\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/10\n",
      "998/998 [==============================] - 155s 156ms/step - loss: 0.0312 - val_loss: 0.0457\n",
      "train_auc: 0.9944 val_auc: 0.9844\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faff0275e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_cv,y_cv),batch_size=128,epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTi5SwR5Hs6_",
    "outputId": "ec529891-2986-4162-8550-54b572caba50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test,\n",
    "                     model_type=\"lstm-glove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVtg8xLIS940"
   },
   "source": [
    "## LSTM Model + Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kccA4OpVTD9b"
   },
   "outputs": [],
   "source": [
    "# loading the fasttext embeddings\n",
    "word_embedding_matrix = load_embeddings(embedding_type=\"fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3va8_hCLTJOl",
    "outputId": "52389572-0ba3-4d27-b439-fa99a61e12eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 300)          23813400  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 128)          186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 24,099,870\n",
      "Trainable params: 286,470\n",
      "Non-trainable params: 23,813,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_lstm_architecture(max_length=max_length,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_matrix=word_embedding_matrix)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iXnBXEpTXWE"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/lstm-glove.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_cv,\n",
    "                              val_labels=y_cv)\n",
    "reduced_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                               patience=1,\n",
    "                               verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=2,\n",
    "                           verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                   monitor=\"val_loss\",\n",
    "                                   save_best_only=True)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,model_checkpoint,reduced_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HRn_hpEUGWS"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72cICNYaUO98",
    "outputId": "0b629e0f-1fbb-42ef-d969-4620a9ffa871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  6/998 [..............................] - ETA: 3:22 - loss: 0.6016WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0948s vs `on_train_batch_end` time: 0.1002s). Check your callbacks.\n",
      "998/998 [==============================] - 169s 158ms/step - loss: 0.0752 - val_loss: 0.0548\n",
      "train_auc: 0.9695 val_auc: 0.9684\n",
      "Epoch 2/10\n",
      "998/998 [==============================] - 156s 156ms/step - loss: 0.0527 - val_loss: 0.0506\n",
      "train_auc: 0.974 val_auc: 0.9733\n",
      "Epoch 3/10\n",
      "998/998 [==============================] - 156s 157ms/step - loss: 0.0499 - val_loss: 0.0481\n",
      "train_auc: 0.9787 val_auc: 0.9779\n",
      "Epoch 4/10\n",
      "998/998 [==============================] - 156s 157ms/step - loss: 0.0475 - val_loss: 0.0489\n",
      "train_auc: 0.981 val_auc: 0.9795\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/10\n",
      "998/998 [==============================] - 156s 156ms/step - loss: 0.0449 - val_loss: 0.0459\n",
      "train_auc: 0.983 val_auc: 0.9814\n",
      "Epoch 6/10\n",
      "998/998 [==============================] - 155s 156ms/step - loss: 0.0442 - val_loss: 0.0458\n",
      "train_auc: 0.9836 val_auc: 0.9819\n",
      "Epoch 7/10\n",
      "998/998 [==============================] - 156s 156ms/step - loss: 0.0438 - val_loss: 0.0457\n",
      "train_auc: 0.984 val_auc: 0.9819\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 8/10\n",
      "998/998 [==============================] - 155s 155ms/step - loss: 0.0434 - val_loss: 0.0457\n",
      "train_auc: 0.9842 val_auc: 0.9821\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 9/10\n",
      "998/998 [==============================] - 157s 157ms/step - loss: 0.0434 - val_loss: 0.0457\n",
      "train_auc: 0.9842 val_auc: 0.9821\n",
      "Epoch 10/10\n",
      "998/998 [==============================] - 156s 156ms/step - loss: 0.0434 - val_loss: 0.0457\n",
      "train_auc: 0.9842 val_auc: 0.9822\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9704bf4d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_cv,y_cv),batch_size=128,epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fc9gzy73oKVS",
    "outputId": "89c1a48f-2ae9-43a1-e86a-a568b128b284",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test,\n",
    "                     model_type=\"lstm-fasttext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAj1DYRcd8G7"
   },
   "source": [
    "## GRU Model + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te-rUNZNeAvw"
   },
   "outputs": [],
   "source": [
    "# loading the glove embeddings\n",
    "word_embedding_matrix = load_embeddings(embedding_type=\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW55JmUXeEat",
    "outputId": "fce5aad6-e459-4e2f-e854-61c8b3bfd977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 300)          23813400  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 128)          140544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 24,029,214\n",
      "Trainable params: 215,814\n",
      "Non-trainable params: 23,813,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_gru_architecture(max_length=max_length,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_matrix=word_embedding_matrix)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Y3sPlgXeWoB"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/gru-glove.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_cv,\n",
    "                              val_labels=y_cv)\n",
    "reduced_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                               patience=1,\n",
    "                               verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=2,\n",
    "                           verbose=1)\n",
    "save_model = SaveModel(file_path=filepath)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,save_model,reduced_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-nxnxPNeZuS"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wi0F0Arqebxu",
    "outputId": "a4676d33-2e62-4104-e139-22c211687616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "998/998 [==============================] - 140s 129ms/step - loss: 0.0611 - val_loss: 0.0461\n",
      "train_auc: 0.982 val_auc: 0.9808\n",
      "Epoch 2/10\n",
      "998/998 [==============================] - 133s 134ms/step - loss: 0.0455 - val_loss: 0.0442\n",
      "train_auc: 0.9868 val_auc: 0.9847\n",
      "Epoch 3/10\n",
      "998/998 [==============================] - 128s 128ms/step - loss: 0.0425 - val_loss: 0.0430\n",
      "train_auc: 0.9894 val_auc: 0.9862\n",
      "Epoch 4/10\n",
      "998/998 [==============================] - 129s 130ms/step - loss: 0.0401 - val_loss: 0.0439\n",
      "train_auc: 0.9911 val_auc: 0.9863\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/10\n",
      "998/998 [==============================] - 139s 140ms/step - loss: 0.0362 - val_loss: 0.0435\n",
      "train_auc: 0.9917 val_auc: 0.9862\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb46c7e1bd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_cv,y_cv),batch_size=128,epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvqoLMkZfcqA",
    "outputId": "6aea4c96-916f-4d33-fe06-717fb017b115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test,\n",
    "                     model_type=\"gru-glove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GHI_H5WLNbO"
   },
   "source": [
    "## GRU Model + Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e9xZJc6LPwt"
   },
   "outputs": [],
   "source": [
    "# loading the fasttext embeddings\n",
    "word_embedding_matrix = load_embeddings(embedding_type=\"fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gV3i1w9dLcXt",
    "outputId": "5901feb2-966b-4a78-8fb4-1581e45fd800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 300)          23813400  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 128)          140544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 24,029,214\n",
      "Trainable params: 215,814\n",
      "Non-trainable params: 23,813,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_gru_architecture(max_length=max_length,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_matrix=word_embedding_matrix)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5ON0rfuLnFq"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/gru-fasttext.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_cv,\n",
    "                              val_labels=y_cv)\n",
    "reduced_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                               patience=1,\n",
    "                               verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=2,\n",
    "                           verbose=1)\n",
    "save_model = SaveModel(file_path=filepath)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,save_model,reduced_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTIYkNX5LpZ1"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jjp2eBZLrhp",
    "outputId": "8d8d672b-902f-4612-ea9f-9e964c102bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "998/998 [==============================] - 141s 130ms/step - loss: 0.0662 - val_loss: 0.0486\n",
      "train_auc: 0.9784 val_auc: 0.977\n",
      "Epoch 2/10\n",
      "998/998 [==============================] - 129s 129ms/step - loss: 0.0475 - val_loss: 0.0457\n",
      "train_auc: 0.983 val_auc: 0.9813\n",
      "Epoch 3/10\n",
      "998/998 [==============================] - 126s 126ms/step - loss: 0.0450 - val_loss: 0.0445\n",
      "train_auc: 0.9867 val_auc: 0.9849\n",
      "Epoch 4/10\n",
      "998/998 [==============================] - 124s 125ms/step - loss: 0.0433 - val_loss: 0.0442\n",
      "train_auc: 0.9881 val_auc: 0.9856\n",
      "Epoch 5/10\n",
      "998/998 [==============================] - 126s 126ms/step - loss: 0.0419 - val_loss: 0.0438\n",
      "train_auc: 0.9895 val_auc: 0.9864\n",
      "Epoch 6/10\n",
      "998/998 [==============================] - 125s 125ms/step - loss: 0.0407 - val_loss: 0.0442\n",
      "train_auc: 0.9904 val_auc: 0.9862\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/10\n",
      "998/998 [==============================] - 124s 125ms/step - loss: 0.0382 - val_loss: 0.0434\n",
      "train_auc: 0.9908 val_auc: 0.9864\n",
      "Epoch 8/10\n",
      "998/998 [==============================] - 124s 125ms/step - loss: 0.0378 - val_loss: 0.0435\n",
      "train_auc: 0.991 val_auc: 0.9864\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 9/10\n",
      "998/998 [==============================] - 127s 127ms/step - loss: 0.0373 - val_loss: 0.0437\n",
      "train_auc: 0.991 val_auc: 0.9863\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf400440d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_cv,y_cv),batch_size=128,epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYKeWhIhL2MS",
    "outputId": "b52ae7b7-f3b6-46c7-e357-7953e7bec4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test,\n",
    "                     model_type=\"gru-fasttext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb6dE2QkicNf"
   },
   "source": [
    "## Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AgbCt0djfJk",
    "outputId": "df95bc91-d7fa-41af-e530-f49741c9fc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# Taking simple average of predictions on test data from all previously trained models\n",
    "\n",
    "# predictions folder file path\n",
    "filepath = \"predictions/\"\n",
    "\n",
    "# class label columns\n",
    "cols = list(pd.read_csv(filepath + \"cnn-glove.csv\", nrows =1))\n",
    "required_cols = [col for col in cols if col != \"id\"]\n",
    "\n",
    "# loading predictions\n",
    "cnn_glove = pd.read_csv(filepath + \"cnn-glove.csv\",usecols=required_cols)\n",
    "cnn_fasttext = pd.read_csv(filepath + \"cnn-fasttext.csv\",usecols=required_cols)\n",
    "lstm_glove = pd.read_csv(filepath + \"lstm-glove.csv\",usecols=required_cols)\n",
    "lstm_fasttext = pd.read_csv(filepath + \"lstm-fasttext.csv\",usecols=required_cols)\n",
    "gru_glove = pd.read_csv(filepath + \"gru-glove.csv\",usecols=required_cols)\n",
    "gru_fasttext = pd.read_csv(filepath + \"gru-fasttext.csv\",usecols=required_cols)\n",
    "\n",
    "# taking average of all model predictions\n",
    "ensemble_predictions = (cnn_glove + cnn_fasttext + lstm_glove + lstm_fasttext + gru_glove + gru_fasttext) / 6\n",
    "\n",
    "# adding ids\n",
    "ids = processed_test[\"id\"]\n",
    "ensemble_predictions[\"id\"] = ids\n",
    "\n",
    "# saving predictions to disk\n",
    "ensemble_predictions.to_csv(filepath + \"ensemble_predictions.csv\")\n",
    "print(\"Predictions saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YgdbuI7j671"
   },
   "source": [
    "# Transfer Learning with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BFvHh2Rj671"
   },
   "source": [
    "Under this section we will be using the pre-trained BERT model to get sentence level embeddings for our comments data. We will be then passing these embeddings through a feed forward neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqKHw5-gj671"
   },
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvRvFCuLpU3I"
   },
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0GO18EzpXLn"
   },
   "outputs": [],
   "source": [
    "def create_tokens(corpus,max_len,tokenizer):\n",
    "    '''\n",
    "    Function to convert sentences into token\n",
    "    representations\n",
    "    '''\n",
    "\n",
    "    tokens_array = []\n",
    "    masked_array = []\n",
    "\n",
    "    for text in corpus:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # truncating excess words\n",
    "        if len(tokens) > max_len - 2:\n",
    "            tokens = tokens[:max_len - 2]\n",
    "        \n",
    "        # adding CLS and SEP special characters\n",
    "        tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "\n",
    "        # padding and masked input\n",
    "        pad_req = max_len - len(tokens)\n",
    "        masked = [1]*len(tokens) + [0]*pad_req\n",
    "        tokens = tokens + ['[PAD]']*pad_req\n",
    "\n",
    "        # converting tokens to ids\n",
    "        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        tokens_array.append(tokens)\n",
    "        masked_array.append(masked)\n",
    "    \n",
    "    # creating segment array\n",
    "    tokens_array = np.array(tokens_array)\n",
    "    masked_array = np.array(masked_array)\n",
    "    segment_array = np.zeros_like(tokens_array)\n",
    "    \n",
    "    return tokens_array, masked_array, segment_array\n",
    "\n",
    "def save_bert_data(x,y,data_type,test=False):\n",
    "    '''\n",
    "    Function to save bert embeddings and data class labels to disk\n",
    "    '''\n",
    "\n",
    "    # specifying file path\n",
    "    filepath = f\"resources/bert/{data_type}.pkl\"\n",
    "\n",
    "    # saving pickle file\n",
    "\n",
    "    if not test:\n",
    "        with open(filepath,mode=\"wb\") as f:\n",
    "            pickle.dump((x,y),f)\n",
    "    else:\n",
    "        with open(filepath,mode=\"wb\") as f:\n",
    "            pickle.dump(x,f)\n",
    "    \n",
    "    print(f\"{data_type} data saved to disk\")\n",
    "\n",
    "\n",
    "def load_bert_data(data_type,test=False):\n",
    "    '''\n",
    "    Function to load saved bert embeddings and class labels from disk\n",
    "    '''\n",
    "\n",
    "    # specifying file path\n",
    "    filepath = f\"resources/bert/{data_type}.pkl\"\n",
    "\n",
    "    # loading pickle file\n",
    "    if not test:\n",
    "      with open(filepath,mode=\"rb\") as f:\n",
    "        x,y = pickle.load(f)\n",
    "        return x,y\n",
    "    else:\n",
    "      with open(filepath,mode=\"rb\") as f:\n",
    "        x = pickle.load(f)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_mlp_architecture(input_dim):\n",
    "    '''\n",
    "    Function returns feed forward neural net architecture\n",
    "    '''\n",
    "\n",
    "    # clearing backend session \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # setting seed for reproducible results\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(0)\n",
    "    rn.seed(0)\n",
    "\n",
    "    # setting initializer and regularizer\n",
    "    initializer = HeNormal()\n",
    "    regularizer = l2(0.0001)\n",
    "\n",
    "    # input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # densely connected layers\n",
    "    dense_1 = Dense(units=512,activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizer)(inputs)\n",
    "    drop_1 = Dropout(rate=0.3)(dense_1)\n",
    "    dense_2 = Dense(units=256,activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizer)(drop_1)\n",
    "    drop_2 = Dropout(rate=0.3)(dense_2)\n",
    "    dense_3 = Dense(units=128,activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizer)(drop_2)\n",
    "    drop_3 = Dropout(rate=0.3)(dense_3)\n",
    "    bn_layer = BatchNormalization()(drop_3)\n",
    "    dense_4 = Dense(units=64,activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizer)(bn_layer)\n",
    "\n",
    "    # output layer\n",
    "    outputs = Dense(units=6,activation='sigmoid',kernel_initializer=initializer)(dense_4)\n",
    "\n",
    "    # defining the model\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU0fOJPNj671"
   },
   "source": [
    "## Defining BERT Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBP4BSWwj671"
   },
   "outputs": [],
   "source": [
    "# clearing session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# maximum sequence length\n",
    "max_seq_length = 200\n",
    "\n",
    "# token inputs\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "\n",
    "# mask arrays\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "\n",
    "# segment arrays\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "#bert layer \n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "\n",
    "# output layer \n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "# bert model\n",
    "bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btAcXp7YnQmF"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiCue20hn6d9"
   },
   "source": [
    "For tokenizing our data before passing it through the BERT model to get the embeddings, we will be using the tokenization script provided by Google's Research Team. \n",
    "\n",
    "https://github.com/google-research/bert/blob/master/tokenization.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mew6MZ-ypqqu"
   },
   "outputs": [],
   "source": [
    "# loading processed train, val and test data \n",
    "with open(\"resources/processed_data.pkl\",\"rb\") as f:\n",
    "    x_train, y_train, x_cv, y_cv, x_test = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di0nbV6onR9x"
   },
   "outputs": [],
   "source": [
    "#getting Vocab file\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFTWNVNspGOS"
   },
   "outputs": [],
   "source": [
    "# importing the tokenization script \n",
    "from resources import tokenization\n",
    "\n",
    "# instantiating tokenizer class\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file,do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IM3cdNZSpPvB"
   },
   "outputs": [],
   "source": [
    "# creating tokenized data along with masks and segments \n",
    "x_train_tokens, x_train_mask, x_train_segment = create_tokens(corpus=x_train, \n",
    "                                                              max_len=200, \n",
    "                                                              tokenizer=tokenizer)\n",
    "\n",
    "x_val_tokens, x_val_mask, x_val_segment = create_tokens(corpus=x_cv, \n",
    "                                                              max_len=200, \n",
    "                                                              tokenizer=tokenizer)\n",
    "\n",
    "x_test_tokens, x_test_mask, x_test_segment = create_tokens(corpus=x_test, \n",
    "                                                              max_len=200, \n",
    "                                                              tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf_SV7IysUtx"
   },
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtwWcyyEsWqm"
   },
   "outputs": [],
   "source": [
    "# getting embeddings of tokenized train, val and test data\n",
    "x_train_bert = bert_model.predict([x_train_tokens, x_train_mask, x_train_segment])\n",
    "x_val_bert = bert_model.predict([x_val_tokens, x_val_mask, x_val_segment])\n",
    "x_test_bert = bert_model.predict([x_test_tokens, x_test_mask, x_test_segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VujSZJSueCG",
    "outputId": "04d0c85b-6bc9-4ff4-8658-8b2b70621fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data saved to disk\n",
      "val data saved to disk\n",
      "test data saved to disk\n"
     ]
    }
   ],
   "source": [
    "# saving the embedding data and labels \n",
    "\n",
    "# train data\n",
    "save_bert_data(x=x_train_bert,\n",
    "               y=y_train,\n",
    "               data_type=\"train\")\n",
    "\n",
    "# val data\n",
    "save_bert_data(x=x_val_bert,\n",
    "               y=y_cv,\n",
    "               data_type=\"val\")\n",
    "\n",
    "# test data\n",
    "save_bert_data(x=x_test_bert,\n",
    "               y=None,\n",
    "               data_type=\"test\",\n",
    "               test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z14q_cbwQdpJ"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh3B6BdvQfex"
   },
   "outputs": [],
   "source": [
    "# loading the embedding data and labels\n",
    "\n",
    "# train data\n",
    "x_train_bert, y_train = load_bert_data(data_type=\"train\")\n",
    "\n",
    "# val data\n",
    "x_val_bert, y_val = load_bert_data(data_type=\"val\")\n",
    "\n",
    "# test data\n",
    "x_test_bert = load_bert_data(data_type=\"test\",\n",
    "                                     test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtG5IFQwRzQ5",
    "outputId": "691e0e4a-0373-4835-ade7-38d753832c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 567,110\n",
      "Trainable params: 566,854\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model architecture\n",
    "model = get_mlp_architecture(input_dim=768)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcDcL07QSiy7"
   },
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "\n",
    "# filepath to save model\n",
    "filepath = \"models/bert-mlp.hdf5\"\n",
    "\n",
    "custom_metric = CustomMetrics(train_data=x_train_bert,\n",
    "                              train_labels=y_train,\n",
    "                              val_data=x_val_bert,\n",
    "                              val_labels=y_val)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=7,\n",
    "                           verbose=1)\n",
    "save_model = SaveModel(file_path=filepath)\n",
    "\n",
    "# adding callbacks to single list\n",
    "callbacks = [custom_metric,early_stop,save_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFUVGQuQTA5S"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhwVheAXTD5f",
    "outputId": "c0118b16-cbbe-4178-f583-ce44f8f56516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "998/998 [==============================] - 11s 7ms/step - loss: 0.4174 - val_loss: 0.2371\n",
      "train_auc: 0.909 val_auc: 0.9112\n",
      "Epoch 2/30\n",
      "998/998 [==============================] - 6s 7ms/step - loss: 0.2281 - val_loss: 0.2002\n",
      "train_auc: 0.9352 val_auc: 0.937\n",
      "Epoch 3/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.2000 - val_loss: 0.1821\n",
      "train_auc: 0.9411 val_auc: 0.941\n",
      "Epoch 4/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1796 - val_loss: 0.1756\n",
      "train_auc: 0.9387 val_auc: 0.9383\n",
      "Epoch 5/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1618 - val_loss: 0.1457\n",
      "train_auc: 0.9499 val_auc: 0.9506\n",
      "Epoch 6/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1462 - val_loss: 0.1337\n",
      "train_auc: 0.9486 val_auc: 0.9482\n",
      "Epoch 7/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1335 - val_loss: 0.1229\n",
      "train_auc: 0.9518 val_auc: 0.9514\n",
      "Epoch 8/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1232 - val_loss: 0.1128\n",
      "train_auc: 0.9533 val_auc: 0.9531\n",
      "Epoch 9/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1151 - val_loss: 0.1069\n",
      "train_auc: 0.9515 val_auc: 0.9517\n",
      "Epoch 10/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1092 - val_loss: 0.1008\n",
      "train_auc: 0.9535 val_auc: 0.9527\n",
      "Epoch 11/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1044 - val_loss: 0.0966\n",
      "train_auc: 0.9543 val_auc: 0.9545\n",
      "Epoch 12/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.1006 - val_loss: 0.0951\n",
      "train_auc: 0.9548 val_auc: 0.9546\n",
      "Epoch 13/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0976 - val_loss: 0.0908\n",
      "train_auc: 0.9545 val_auc: 0.9542\n",
      "Epoch 14/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0953 - val_loss: 0.0895\n",
      "train_auc: 0.9544 val_auc: 0.9544\n",
      "Epoch 15/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0927 - val_loss: 0.0892\n",
      "train_auc: 0.9547 val_auc: 0.9543\n",
      "Epoch 16/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0913 - val_loss: 0.0868\n",
      "train_auc: 0.9555 val_auc: 0.9546\n",
      "Epoch 17/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0899 - val_loss: 0.0843\n",
      "train_auc: 0.9561 val_auc: 0.9553\n",
      "Epoch 18/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0882 - val_loss: 0.0839\n",
      "train_auc: 0.9562 val_auc: 0.9559\n",
      "Epoch 19/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0873 - val_loss: 0.0815\n",
      "train_auc: 0.9569 val_auc: 0.9564\n",
      "Epoch 20/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0857 - val_loss: 0.0831\n",
      "train_auc: 0.9563 val_auc: 0.9556\n",
      "Epoch 21/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0850 - val_loss: 0.0811\n",
      "train_auc: 0.9552 val_auc: 0.9546\n",
      "Epoch 22/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0842 - val_loss: 0.0797\n",
      "train_auc: 0.9568 val_auc: 0.9568\n",
      "Epoch 23/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0833 - val_loss: 0.0823\n",
      "train_auc: 0.9558 val_auc: 0.9553\n",
      "Epoch 24/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0830 - val_loss: 0.0797\n",
      "train_auc: 0.9571 val_auc: 0.9564\n",
      "Epoch 25/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0823 - val_loss: 0.0779\n",
      "train_auc: 0.9558 val_auc: 0.9553\n",
      "Epoch 26/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0816 - val_loss: 0.0778\n",
      "train_auc: 0.9572 val_auc: 0.9566\n",
      "Epoch 27/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0808 - val_loss: 0.0760\n",
      "train_auc: 0.9586 val_auc: 0.9584\n",
      "Epoch 28/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0805 - val_loss: 0.0764\n",
      "train_auc: 0.9582 val_auc: 0.9579\n",
      "Epoch 29/30\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 0.0796 - val_loss: 0.0761\n",
      "train_auc: 0.9589 val_auc: 0.9583\n",
      "Epoch 30/30\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 0.0797 - val_loss: 0.0753\n",
      "train_auc: 0.9585 val_auc: 0.958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc02060d790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_bert,y_train,validation_data=(x_val_bert,y_val),batch_size=128,epochs=30,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgBqvwaZVwNI",
    "outputId": "44110f36-d13d-4763-fb3f-560f4896cb34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to disk\n"
     ]
    }
   ],
   "source": [
    "# predicting on test data\n",
    "get_test_predictions(test_ids=processed_test[\"id\"],\n",
    "                     test_data=x_test_bert,\n",
    "                     model_type=\"bert-mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jxHWBh4yglH"
   },
   "source": [
    "## Summarizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUbxPq4_yjsj",
    "outputId": "5ccbe738-6d8d-4a87-bef8-c796d456352c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------+-----------+----------------------+\n",
      "|           Model           | Train Score | Val Score | Kaggle Private Score |\n",
      "+---------------------------+-------------+-----------+----------------------+\n",
      "|         CNN-GloVe         |    0.9862   |   0.9725  |        0.9629        |\n",
      "|        CNN-Fasttext       |    0.9878   |   0.9688  |        0.9595        |\n",
      "|         LSTM-GloVe        |    0.9944   |   0.9844  |        0.9792        |\n",
      "|       LSTM-Fasttext       |    0.9842   |   0.9822  |        0.9765        |\n",
      "|         GRU-GloVe         |    0.9917   |   0.9862  |        0.9799        |\n",
      "|        GRU-Fasttext       |    0.9911   |   0.9863  |        0.9806        |\n",
      "| Ensemble (Simple Average) |      --     |     --    |        0.9809        |\n",
      "|          BERT-MLP         |    0.9585   |   0.9581  |        0.9461        |\n",
      "+---------------------------+-------------+-----------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# creating table object\n",
    "table = PrettyTable(field_names=[\"Model\",\"Train Score\",\"Val Score\",\"Kaggle Private Score\"])\n",
    "\n",
    "# adding rows\n",
    "table.add_row([\"CNN-GloVe\",0.9862,0.9725,0.9629])\n",
    "table.add_row([\"CNN-Fasttext\",0.9878,0.9688,0.9595])\n",
    "table.add_row([\"LSTM-GloVe\",0.9944,0.9844,0.9792])\n",
    "table.add_row([\"LSTM-Fasttext\",0.9842,0.9822,0.9765])\n",
    "table.add_row([\"GRU-GloVe\",0.9917,0.9862,0.9799])\n",
    "table.add_row([\"GRU-Fasttext\",0.9911,0.9863,0.9806])\n",
    "table.add_row([\"Ensemble (Simple Average)\",\"--\",\"--\",0.9809])\n",
    "table.add_row([\"BERT-MLP\",0.9585,0.9581,0.9461])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "m4HMqo-1n7er",
    "9oMJo1iLn7er",
    "KIns_Pgjn7er",
    "b2rg1z_in7es",
    "qu_qj9Oon7et",
    "jVpPMqz5n7et",
    "YvsiMVnLn7eu",
    "3rIvQChkn7ev",
    "Ziuz2Ifvn7ew",
    "gtzyO1xSn7ew"
   ],
   "name": "EDA_Processing_Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
